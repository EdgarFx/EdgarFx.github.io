<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Computer Vision and Robotics | Xiang (Edgar) Fei</title>
    <link>https://EdgarFx.github.io/tag/computer-vision-and-robotics/</link>
      <atom:link href="https://EdgarFx.github.io/tag/computer-vision-and-robotics/index.xml" rel="self" type="application/rss+xml" />
    <description>Computer Vision and Robotics</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Fri, 04 Aug 2023 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://EdgarFx.github.io/media/icon_hu511fa36bc4ff2d03adc724be90fee2b8_24177_512x512_fill_lanczos_center_3.png</url>
      <title>Computer Vision and Robotics</title>
      <link>https://EdgarFx.github.io/tag/computer-vision-and-robotics/</link>
    </image>
    
    <item>
      <title>Bag-of-Word-Groups (BoWG), A Robust Loop Closure Module for In-pipe Visual-Laser-Inertial SLAM</title>
      <link>https://EdgarFx.github.io/research/bowg/</link>
      <pubDate>Fri, 04 Aug 2023 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/research/bowg/</guid>
      <description>&lt;p&gt;In-pipe simultaneous localization and mapping (SLAM) techniques with photorealistic RGB-D reconstruction capability have the potential to enhance human labor to inspect pipe conditions and localize anomalies, thereby preventing hazardous leaks and explosions. Loop closure detection is vital in the process of SLAM, as it helps reduce the accumulative drift of the robotâ€™s estimated odometry and generate a globally consistent map. However, in confined-space environments such as narrow pipes, conventional loop closure methods suffer perceptual aliasing due to feature scarcity and textural repetitiveness. In this research, we aim to develop a robust loop closure module in confined-space environments on top of our prior confined-space dense RGB-D SLAM method, visual-laser-inertial (VLI) SLAM. Specifically, we define the concept of word group based on spatial proximity and positions of features and propose to build and maintain a novel loop closure detection module called Bag-of-Word-Groups (BoWG) online, which provides context-specific feature representation. Besides, we utilize Gaussian pyramids to implement Multi-scale Good Features To Track (MS-GFTT) to detect richer features at various scales for word group analysis. Our method does not require any extra sensor other than a monocular visual camera and can be easily integrated into existing Bag-of-Words (BoW) methods. To validate the proposed method, we conduct real-world experiments in a narrow, feature-sparse pipeline with loops. Experiment results show that our method is robust and can achieve high precision while maintaining acceptable recall when the perceptual aliasing problem is serious. In addition, the proposed method has the potential to be applied to environments other than narrow pipes.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>SLAM for Vision-based Navigation</title>
      <link>https://EdgarFx.github.io/research/navigation/</link>
      <pubDate>Fri, 14 Apr 2023 10:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/research/navigation/</guid>
      <description>&lt;p&gt;This research project is initiated by Prof. Shankar Sastry (UC Berkeley) and Prof. Somil Bansal (University of Southern California). The main aim of the research is to create a new autonomous navigation pipeline that incorporates learning-based perception for extracting distinctive features and semantic interpretation, and model-based SLAM for inference and metric map reconstruction. Specifically, learning-based perception techniques have the ability to successfully identify objects that are important for the subsequent navigation task, but may face difficulties in accurately reconstructing metrics in challenging scenarios such as narrow hallways and tight corners. On the other hand, visual SLAM methods can produce consistent metric maps but often require significant computational resources, and do not provide relevant semantic information. This study aims to explore methods of integrating these two approaches, learning-based and model-based, to establish an efficient and reliable autonomous navigation system that can achieve high success rates and adapt to complex and unpredictable environments.&lt;/p&gt;
</description>
    </item>
    
  </channel>
</rss>
