<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Xiang (Shawn) Fei</title>
    <link>https://EdgarFx.github.io/</link>
      <atom:link href="https://EdgarFx.github.io/index.xml" rel="self" type="application/rss+xml" />
    <description>Xiang (Shawn) Fei</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><lastBuildDate>Sat, 01 Jun 2030 13:00:00 +0000</lastBuildDate>
    <image>
      <url>https://EdgarFx.github.io/media/icon_hu511fa36bc4ff2d03adc724be90fee2b8_24177_512x512_fill_lanczos_center_3.png</url>
      <title>Xiang (Shawn) Fei</title>
      <link>https://EdgarFx.github.io/</link>
    </image>
    
    <item>
      <title>Example Talk</title>
      <link>https://EdgarFx.github.io/talk/example-talk/</link>
      <pubDate>Sat, 01 Jun 2030 13:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/talk/example-talk/</guid>
      <description>&lt;div class=&#34;alert alert-note&#34;&gt;
  &lt;div&gt;
    Click on the &lt;strong&gt;Slides&lt;/strong&gt; button above to view the built-in slides feature.
  &lt;/div&gt;
&lt;/div&gt;
&lt;p&gt;Slides can be added in a few ways:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Create&lt;/strong&gt; slides using Wowchemy&amp;rsquo;s &lt;a href=&#34;https://wowchemy.com/docs/managing-content/#create-slides&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;em&gt;Slides&lt;/em&gt;&lt;/a&gt; feature and link using &lt;code&gt;slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Upload&lt;/strong&gt; an existing slide deck to &lt;code&gt;static/&lt;/code&gt; and link using &lt;code&gt;url_slides&lt;/code&gt; parameter in the front matter of the talk file&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Embed&lt;/strong&gt; your slides (e.g. Google Slides) or presentation video on this page using &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;shortcodes&lt;/a&gt;.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Further event details, including &lt;a href=&#34;https://wowchemy.com/docs/writing-markdown-latex/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;page elements&lt;/a&gt; such as image galleries, can be added to the body of this page.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Bag of Word Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing</title>
      <link>https://EdgarFx.github.io/research/bowg/</link>
      <pubDate>Sun, 15 Jun 2025 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/research/bowg/</guid>
      <description>&lt;p&gt;Loop closure is critical in Simultaneous Localization and Mapping (SLAM) systems to reduce accumulative drift and ensure global mapping consistency. However, conventional methods struggle in perceptually aliased environments, such as narrow pipes, due to vector quantization, feature sparsity, and repetitive textures, while existing solutions often incur high computational costs. This paper presents Bag-of-Word-Groups (BoWG), a novel loop closure detection method that achieves superior precision-recall, robustness, and computational efficiency. The core innovation lies in the introduction of word groups, which captures the spatial co-occurrence and proximity of visual words to construct an online dictionary. Additionally, drawing inspiration from probabilistic transition models, we incorporate temporal consistency directly into similarity computation with an adaptive scheme, substantially improving precision-recall performance. The method is further strengthened by a feature distribution analysis module and dedicated post-verification mechanisms. To evaluate the effectiveness of our method, we conduct experiments on both public datasets and a confined-pipe dataset we constructed. Results demonstrate that BoWG surpasses state-of-the-art methods‚Äîincluding both traditional and learning-based approaches‚Äîin terms of precision-recall and computational efficiency. Our approach also exhibits excellent scalability, achieving an average processing time of 16 ms per image across 17,565 images in the Bicocca25b dataset. The source code is available at: &lt;a href=&#34;https://github.com/EdgarFx/BoWG&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://github.com/EdgarFx/BoWG&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Carbon Market Risk Estimation Using Quantum Conditional Generative Adversarial Network and Amplitude Estimation</title>
      <link>https://EdgarFx.github.io/publication/q_gan/</link>
      <pubDate>Thu, 08 Aug 2024 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/publication/q_gan/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Power System Fault Diagnosis with Quantum Computing and Efficient Gate Decomposition</title>
      <link>https://EdgarFx.github.io/publication/q_optimization/</link>
      <pubDate>Tue, 23 Jul 2024 00:12:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/publication/q_optimization/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Photorealistic-3D-Reconstruction-with-Multi-view-Stereo</title>
      <link>https://EdgarFx.github.io/project/real_acmm/</link>
      <pubDate>Sat, 20 Jan 2024 10:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/project/real_acmm/</guid>
      <description>&lt;p&gt;In this research project, we propose real-ACMM, which is a photorealistic 3D reconstruction method with MVS. The proposed real-ACMM is based on the architecture of &lt;a href=&#34;https://arxiv.org/abs/1904.08103&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;ACMM&lt;/a&gt;, and we make improvements on ACMM, achieving better reconstruction results with fewer iterations, especially in low-texture areas. The &lt;a href=&#34;https://github.com/EdgarFx/Photorealistic-3D-Reconstruction-with-Multi-view-Stereo&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; is open-sourced.&lt;/p&gt;
&lt;p&gt;Our main contributions are:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Proposed Broad Adaptive Checkerboard Sampling, which broadly considers all the pixels in a neighborhood window during pixel sampling, instead of extending in a specific direction. This method helps capture correct hypotheses in large low-texture areas.&lt;/li&gt;
&lt;li&gt;Introduced Dynamic Multi-Hypothesis Joint View Selection, which dynamically adjusts the matching cost for both the good matching and bad matching, allowing more robust and accurate view selection.&lt;/li&gt;
&lt;li&gt;Results show that the proposed method can achieve better reconstruction results with fewer iterations, especially in low-texture areas.&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;results-in-eth3d-benchmark&#34;&gt;Results in ETH3D benchmark&lt;/h2&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Depth map comparison between different algorithms on ETH3D pipes dataset&#34; srcset=&#34;
               /project/real_acmm/depth_hu245772cc57a08179b058337bb5baab20_972865_d11b35918b5b4850610dfa6a2ff18d5f.webp 400w,
               /project/real_acmm/depth_hu245772cc57a08179b058337bb5baab20_972865_4472c77b2333ef817d3c8c09442cf226.webp 760w,
               /project/real_acmm/depth_hu245772cc57a08179b058337bb5baab20_972865_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://EdgarFx.github.io/project/real_acmm/depth_hu245772cc57a08179b058337bb5baab20_972865_d11b35918b5b4850610dfa6a2ff18d5f.webp&#34;
               width=&#34;760&#34;
               height=&#34;442&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;Point cloud comparisons between different algorithms on ETH3D pipes dataset&#34; srcset=&#34;
               /project/real_acmm/point_hu7b5e6900ffe116fd3c12ad820a0de0fa_919960_04e92d51044f0a1407c33d485ff61ff4.webp 400w,
               /project/real_acmm/point_hu7b5e6900ffe116fd3c12ad820a0de0fa_919960_115e2ed6bf5c36be5a104a51a5513fdd.webp 760w,
               /project/real_acmm/point_hu7b5e6900ffe116fd3c12ad820a0de0fa_919960_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://EdgarFx.github.io/project/real_acmm/point_hu7b5e6900ffe116fd3c12ad820a0de0fa_919960_04e92d51044f0a1407c33d485ff61ff4.webp&#34;
               width=&#34;760&#34;
               height=&#34;431&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Linear-Layer-Enhanced Quantum Long Short-Term Memory for Carbon Price Forecasting</title>
      <link>https://EdgarFx.github.io/publication/l_qlstm/</link>
      <pubDate>Wed, 05 Jul 2023 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/publication/l_qlstm/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Linear-Layer-Enhanced Quantum Long Short-Term Memory for Carbon Price Forecasting</title>
      <link>https://EdgarFx.github.io/research/l_qlstm/</link>
      <pubDate>Sun, 14 May 2023 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/research/l_qlstm/</guid>
      <description>&lt;p&gt;Accurate carbon price forecasting is important for investors and policymakers to make decisions in the carbon market. With the development of quantum computing in recent years, quantum machine learning has shown great potential in a wide range of areas. The aim of this research theme is to try to adapt Quantum Long Short-Term Memory to forecast the carbon price.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Power System Fault Diagnosis with Quantum Computing and Efficient Gate Decomposition</title>
      <link>https://EdgarFx.github.io/research/q_optimization/</link>
      <pubDate>Sat, 15 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/research/q_optimization/</guid>
      <description>&lt;p&gt;Power system fault diagnosis helps to reproduce the fault process and provide the dispatchers with a basis for fault decision-making. The aim of this research theme is to try to adapt quantum approximate optimization algorithm to solve power system fault diagnosis problem under current hardware limit, utilizing the potential advantages of quantum computing to handle more complex power system faults.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Carbon Disclosure Effect, Corporate Fundamentals, and Net-zero Emission Target: Evidence from China</title>
      <link>https://EdgarFx.github.io/publication/disclosure/</link>
      <pubDate>Fri, 14 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/publication/disclosure/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Carbon Disclosure Effect, Corporate Fundamentals, and Net-zero Emission Target: Evidence from China</title>
      <link>https://EdgarFx.github.io/research/disclosure/</link>
      <pubDate>Fri, 14 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/research/disclosure/</guid>
      <description>&lt;p&gt;Under the national net-zero emission target, carbon disclosure becomes more significant for investors to evaluate corporate climate risk in formulating investing strategies. This study aims to analyze the impact of Carbon Disclosure on Company Financial Performance.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Carbon Disclosure Effect, Corporate Fundamentals, and Net-zero Emission Target: Evidence from China</title>
      <link>https://EdgarFx.github.io/research/ets/</link>
      <pubDate>Thu, 13 Apr 2023 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/research/ets/</guid>
      <description>&lt;p&gt;This paper conducts an empirical analysis of the impact of China‚Äôs national and regional Emissions Trading Scheme (ETS) on the Ô¨Ånancial performance of listed companies. A comprehensive dataset is constructed, encompassing Ô¨Årm-level carbon allowance data and Ô¨Årm-level emission data from July 2021 to December 2022. Our Ô¨Åndings indicate that, in the initial years of China‚Äôs ETS implementation, companies participating in the national ETS experienced a statistically signiÔ¨Åcant superior performance compared to non-participating Ô¨Årms due to excess free carbon emission allowances. Results also show that National ETS resulted in a statistically signiÔ¨Åcant reduction in emissions of regulated companies. Nevertheless, involvement in the regional ETS yielded no discernible e‚Üµect on the Ô¨Ånancial and environmental performance of regulated entities. The outcomes reveal the existence of a substantial and statistically signiÔ¨Åcant ‚Äùover-allocation of carbon allowances,‚Äù predominantly attributable to increased non-operational income of regulated companies in China‚Äôs national ETS.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Simulation Modeling Analysis of Carbon-Electricity Market in the Context of Carbon Neutrality</title>
      <link>https://EdgarFx.github.io/research/simulation/</link>
      <pubDate>Sat, 04 Mar 2023 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/research/simulation/</guid>
      <description>&lt;p&gt;This paper studied carbon market clearing, electricity market clearing, and computable general equilibrium (CGE) models to simulate carbon, electricity, and macro-economy systems. A hybrid experimental learning method is applied to simulate the carbon-electricity market, which solves the macro-scale problems from the conflict between electricity market price reduction and carbon emission cost, and the micro-scale problems from the different decision time scales. Based on the simulation framework, we can study the electricity market forecasting analysis techniques. Based on the Guangdong trial spot market, a case study on a 3-bus electricity market is launched to test the proposed method&amp;rsquo;s adaption in the bidding behavior modeling of the electricity market. Moreover, a CGE experiment finds out that the carbon market can reduce the emission amount of the eight industries. And the carbon price can affect each industry&amp;rsquo;s emission amount, however, it has little effect on the total amount. This work is supported by The Science and Technology Project of State Grid Zhejiang Electric Power Co., Ltd. ‚ÄúResearch on Carbon Trading System Based on Blockchain Technology‚Äù (Grant NO.5211LS21N002).&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>3D Reconstruction of the Gastrointestinal Tract and Sinus Surface</title>
      <link>https://EdgarFx.github.io/project/winning/</link>
      <pubDate>Thu, 01 Sep 2022 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/project/winning/</guid>
      <description>&lt;h2 id=&#34;overview&#34;&gt;Overview&lt;/h2&gt;
&lt;p&gt;This task involved creating 3D reconstructions of the gastrointestinal tract and sinus surface using a monocular endoscope. The main goal of this project was to document the endoscopy process in 3D and to help doctors realize a more comprehensive examination. Given the project&amp;rsquo;s emphasis on reconstruction accuracy over real-time operation, I opted to explore methods rooted in Structure from Motion (SfM) instead of Simultaneous Localization and Mapping (SLAM). During the reconstruction process within a structure like the human gastrointestinal tract and sinus surface, various challenges arose, such as textureless image frames, varying lighting conditions, and contaminated frames. To address these issues, I employed learning-based approaches. Utilizing the sparse reconstruction and camera pose obtained by SfM with SIFT as self-supervised signals, I trained a two-branch Siamese network to achieve dense depth estimation and feature descriptors. Following this, depth fusion and surface extraction were performed to reconstruct a highly accurate watertight triangle mesh surface of the gastrointestinal tract and sinus surface. There, I was responsible for methodology conceptualization and code implementation.&lt;/p&gt;
&lt;center class=&#34;half&#34;&gt;
    &lt;img src=&#34;https://media.giphy.com/media/TRSYjA7bffmxDtONdy/giphy.gif&#34; width=&#34;70%&#34;/&gt;
    &lt;figcaption&gt;
					 &lt;b&gt;Figure 1.&lt;/b&gt; Point cloud obtained by the dense depth estimation.
    &lt;/figcaption&gt;
&lt;/center&gt;
&lt;center class=&#34;half&#34;&gt;
    &lt;img src=&#34;https://media.giphy.com/media/ZI4METZ0dPjbWVlCOz/giphy.gif&#34; width=&#34;70%&#34;/&gt;
    &lt;figcaption&gt;
					 &lt;b&gt;Figure 2.&lt;/b&gt; The overlay of the video and the point cloud obtain by COLMAP with the dense feature descriptors.
    &lt;/figcaption&gt;
&lt;/center&gt;
&lt;h2 id=&#34;more-details-about-learning-based-dense-depth-estimation-and-feature-descriptors&#34;&gt;More Details about Learning-based Dense Depth Estimation and Feature Descriptors&lt;/h2&gt;
&lt;h3 id=&#34;1-dense-depth-estimation&#34;&gt;1. Dense Depth Estimation&lt;/h3&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/winning/images/2022-08-29-09-32-00_hu3b0edff42ef0ce742708ada079f1d724_206294_b238d583c296c7f49d22be13d8f7d1c6.webp 400w,
               /project/winning/images/2022-08-29-09-32-00_hu3b0edff42ef0ce742708ada079f1d724_206294_76a5bf1bbdbfbaacdbb82d6783d94914.webp 760w,
               /project/winning/images/2022-08-29-09-32-00_hu3b0edff42ef0ce742708ada079f1d724_206294_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://EdgarFx.github.io/project/winning/images/2022-08-29-09-32-00_hu3b0edff42ef0ce742708ada079f1d724_206294_b238d583c296c7f49d22be13d8f7d1c6.webp&#34;
               width=&#34;760&#34;
               height=&#34;460&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The figure depicts the architecture of a dense depth estimation network. In general, the network training relies on a loss function to update network parameters by backpropagating useful information in the form of gradients. The loss function comprises the introduced components: Sparse Flow Loss and Depth Consistency Loss. To guide the training of depth estimation using these losses, several types of input data are necessary. These inputs include endoscopic video frames, camera poses and intrinsic parameters, sparse depth maps, sparse soft masks, and sparse flow maps, elaborated in the training data section. Finally, in order to transform the network predictions obtained from Monocular Depth Estimation into an appropriate format for loss computation, several custom layers are employed. These custom layers include the Depth Scaling Layer, Depth Warping Layer, and Flow from Depth Layer, detailed in the network architecture section.&lt;/p&gt;
&lt;h4 id=&#34;11-training-data&#34;&gt;1.1 Training Data&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/winning/images/2022-08-29-09-40-26_hub5c782196323c81a1b3303204d49716d_198039_14c9ff23e5501bf6d744e3d85785df98.webp 400w,
               /project/winning/images/2022-08-29-09-40-26_hub5c782196323c81a1b3303204d49716d_198039_427fdb2e758b0e13c52313d38edfd020.webp 760w,
               /project/winning/images/2022-08-29-09-40-26_hub5c782196323c81a1b3303204d49716d_198039_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://EdgarFx.github.io/project/winning/images/2022-08-29-09-40-26_hub5c782196323c81a1b3303204d49716d_198039_14c9ff23e5501bf6d744e3d85785df98.webp&#34;
               width=&#34;760&#34;
               height=&#34;321&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The training data originates from unlabeled endoscopic videos, as illustrated in the framework above.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Data Preprocessing&lt;/strong&gt;: Initially, the video sequences undergo undistortion using distortion coefficients estimated from the respective calibrated videos. Sparse reconstruction, camera poses, and point visibility are estimated by Structure from Motion (SfM) from the undistorted video sequences, excluding black invalid regions within video frames. Point cloud filtering is employed to remove extreme outliers from the sparse reconstruction. To smooth the point visibility information by leveraging continuous camera movement in the video, as depicted in Figure b, is utilized. The sparse form data generated from SfM results is elaborated below.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sparse Depth Maps&lt;/strong&gt;: Monocular Depth Estimation solely predicts depth on a global scale. However, for effective loss computation, the scale of depth prediction must align with the SfM results. Hence, the introduced sparse depth maps here serve as anchors for scaling depth predictions in the Depth Scaling Layer. To generate sparse depth maps, 3D points from sparse reconstruction by SfM are projected onto the image plane with camera poses, intrinsic functions, and point visibility information.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sparse Flow Maps&lt;/strong&gt;: Sparse flow maps are utilized for the subsequent Sparse Flow Loss (SFL). Previously, we directly used sparse depth maps for loss computation to leverage self-supervised signals from sparse reconstruction. This led to fixed and potentially biased training targets, such as sparse depth maps, per frame. In contrast to sparse depth maps, sparse flow maps describe 2D projection motion of sparse reconstruction involving camera poses of two input frames with random frame intervals. By combining camera trajectories and sparse reconstruction and considering all pairs of frame combinations, errors in new targets, like sparse flow maps, are likely unbiased per frame. This reduces the influence of random noise in training data on the network. For models trained using SFL, depth predictions exhibit natural smoothness and edge preservation, eliminating the need for explicit regularization during training.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Sparse Soft Masks&lt;/strong&gt;ÔºöSparse masks enable the network to utilize effective sparse signals within sparse form data while disregarding other invalid areas. Soft weighting defined before training interprets the fact that the error distribution of individual points in SfM results varies and mitigates the impact of SfM reconstruction errors. The intuition behind this design is that the more frames used in triangulating a 3D point in SfM bundle adjustment, the higher the precision usually attained. Sparse soft masks are employed in the subsequent SFL.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;12-network-architecture&#34;&gt;1.2 Network Architecture&lt;/h4&gt;
&lt;p&gt;The overall network architecture during the training phase consists of a Siamese dual-branch network. It relies on sparse signals from SfM and geometric constraints between two frames to learn predicting dense depth maps from a single endoscopic video frame. During the application phase, the network employs a simpler single-branch architecture for depth estimation from a single frame. All the described custom layers below are differentiable, enabling the network to be trained end-to-end.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Monocular Depth Estimation&lt;/strong&gt;: This module utilizes an architecture called DenseNet, achieving performance comparable to other popular architectures while significantly reducing network parameters by extensively reusing previous feature maps. The channel count of the final convolutional layer is altered to 1, and the final activations, i.e., log-softmax and linear activations, are replaced to suit the task of depth prediction. The transposed convolutional layers in the upscaling section of the network are substituted with nearest-neighbor upsampling and convolutional layers to reduce checkerboard artifacts in the final output.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Depth Scaling Layer&lt;/strong&gt;: This layer scales the depth predictions of Monocular Depth Estimation to match the scale of the corresponding SfM results for correct loss computation.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Flow from Depth Layer&lt;/strong&gt;: To guide the network training using the Sparse Flow Loss (SFL) described later with the sparse flow maps generated from SfM results, it is necessary to convert the scaled depth maps into dense flow maps with relative camera poses and intrinsic matrices. The resultant dense flow maps are used for depth estimation training, essentially describing a 2D displacement field for 3D viewpoint changes.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Depth Warping Layer&lt;/strong&gt;: The sparse flow maps primarily guide regions projected from sparse information in the frames from SfM. As most frames have only a small fraction of pixel values effectively represented in the sparse flow maps, most regions remain inadequately guided. Leveraging geometric constraints between two frames through camera motion and intrinsic parameters enforces consistency between two corresponding depth predictions. The intuition is that densely predicted depth maps from two adjacent frames are related due to observed overlapping regions. To ensure differentiable implementation of the geometric constraints employed in the subsequent Depth Consistency Loss, alignment of viewpoint for depth predictions is prerequisite.&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;13-loss-functions&#34;&gt;1.3 Loss Functions&lt;/h4&gt;
&lt;p&gt;The newly designed losses leverage self-supervised signals from SfM and enforce geometric consistency between depth predictions of two frames.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;strong&gt;Sparse Flow Loss (SFL)&lt;/strong&gt;: To generate accurate dense depth maps consistent with sparse reconstructions from SfM, the network is trained to minimize differences between dense flow maps and corresponding sparse flow maps. This loss is scale-invariant, considering differences in 2D projection motion in pixels, resolving data imbalance issues arising from arbitrary scales in SfM results.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Depth Consistency Loss (DCL)&lt;/strong&gt;: Solely relying on sparse signals from SFL doesn&amp;rsquo;t provide sufficient information for the network to infer regions without available sparse annotations. Therefore, geometric constraints are enforced between two independently predicted depth maps.&lt;/li&gt;
&lt;li&gt;&lt;strong&gt;Overall Loss&lt;/strong&gt;: The comprehensive loss function for training the network using a pair of training data from frames $j$ and $k$ is represented as: $$L(j,k)=\lambda_1L_{flow}(j,k)+\lambda_2L_{consist}(j,k)$$&lt;/li&gt;
&lt;/ul&gt;
&lt;h4 id=&#34;14-experiment&#34;&gt;1.4 Experiment&lt;/h4&gt;
&lt;ul&gt;
&lt;li&gt;Run 100 epochsÔºåvalidation loss is as follows.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/winning/images/2022-08-29-10-33-30_hu96719d4517007a6aad8ec34705e89010_106136_fccec279c0edcd9f85b475557500596f.webp 400w,
               /project/winning/images/2022-08-29-10-33-30_hu96719d4517007a6aad8ec34705e89010_106136_86182e8ae1e8b51556c60245097ca543.webp 760w,
               /project/winning/images/2022-08-29-10-33-30_hu96719d4517007a6aad8ec34705e89010_106136_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://EdgarFx.github.io/project/winning/images/2022-08-29-10-33-30_hu96719d4517007a6aad8ec34705e89010_106136_fccec279c0edcd9f85b475557500596f.webp&#34;
               width=&#34;760&#34;
               height=&#34;312&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The reason why there is a sudden increase here is because the weight of depth consistency loss is always set to 0.1 in the first 20 epochs, and is set to 5 after 20 epochs.&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;The dense depth map predicted by the validation set is shown in the figure below.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/winning/images/2022-08-29-10-36-09_huce74c7c1f30768e870210a8a50a695b8_406029_e81088ac435ebc9bae0fa51c8f866b8a.webp 400w,
               /project/winning/images/2022-08-29-10-36-09_huce74c7c1f30768e870210a8a50a695b8_406029_37534a362937f5c37aedffc8b3d73710.webp 760w,
               /project/winning/images/2022-08-29-10-36-09_huce74c7c1f30768e870210a8a50a695b8_406029_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://EdgarFx.github.io/project/winning/images/2022-08-29-10-36-09_huce74c7c1f30768e870210a8a50a695b8_406029_e81088ac435ebc9bae0fa51c8f866b8a.webp&#34;
               width=&#34;760&#34;
               height=&#34;491&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;h3 id=&#34;2-dense-feature-descriptors&#34;&gt;2. Dense Feature Descriptors&lt;/h3&gt;
&lt;h4 id=&#34;21-overall-network-architecture&#34;&gt;2.1 Overall Network Architecture&lt;/h4&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/winning/images/2022-08-29-13-28-51_hu9c968ff88a86c5e100ebae24af2a5068_86604_9d8b367e5332a42d769cc5211f01f20c.webp 400w,
               /project/winning/images/2022-08-29-13-28-51_hu9c968ff88a86c5e100ebae24af2a5068_86604_2448e4f68ed38ee0a8e94a5040b7d20e.webp 760w,
               /project/winning/images/2022-08-29-13-28-51_hu9c968ff88a86c5e100ebae24af2a5068_86604_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://EdgarFx.github.io/project/winning/images/2022-08-29-13-28-51_hu9c968ff88a86c5e100ebae24af2a5068_86604_9d8b367e5332a42d769cc5211f01f20c.webp&#34;
               width=&#34;760&#34;
               height=&#34;277&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;As depicted in the figure above, the training network comprises a Siamese dual-branch architecture. It takes in a pair of color images, serving as the Source and Target, respectively. The training objective is to identify the correct corresponding keypoint positions in the Target image, given the keypoint locations in the Source image. The SfM method, incorporating SIFT, is applied to video sequences to estimate sparse 3D reconstructions and camera poses. Subsequently, the sparse 3D reconstruction is projected onto the image plane using the estimated camera poses to generate ground truth point correspondences. The dense feature extraction module is a fully convolutional DenseNet that receives color images and outputs dense descriptor mappings with the same resolution as the input images, employing the length of the feature descriptors as the channel dimension. The descriptor mappings are L2-normalized along the channel dimension to enhance generalization. For each source keypoint location, corresponding descriptors are sampled from the source descriptor mapping. These descriptors of source keypoints serve as 1√ó1 convolutional kernels to perform 2D convolutions on the target descriptor mapping in the Point-of-Interest (POI) convolutional layer. The computed heatmap represents the similarity between the source keypoint locations and every position on the target image. The network is trained using the suggested Relative Response (RR) loss to enforce the heatmap to exhibit high responses solely at the true target positions.&lt;/p&gt;
&lt;h4 id=&#34;22-point-of-interest-poi-conv-layer&#34;&gt;2.2 Point-of-Interest (POI) Conv Layer&lt;/h4&gt;
&lt;p&gt;This layer serves to transform the descriptor learning problem into keypoint localization. For a pair of source and target input images, a pair of dense descriptor mappings $F_s$ and $F_t$ are generated from the feature extraction module. The sizes of the input images and descriptor mappings are $3 √ó H √ó W$ and $C √ó H √ó W$, respectively. For a descriptor at the source keypoint position $x_s$, the corresponding feature descriptor $F_s(x_s)$ is extracted using nearest neighbor sampling, which can be altered if needed to employ other sampling methods. The descriptor size is $C √ó 1 √ó 1$. By treating the sampled feature descriptor as a $1√ó1$ convolutional kernel, a 2D convolution operation is performed on $F_t$ to generate the target heatmap $M_t$, storing the similarity between the source descriptor and each target descriptor in $F_t$.&lt;/p&gt;
&lt;h4 id=&#34;23-relative-response-loss-rr&#34;&gt;2.3 Relative Response Loss (RR)&lt;/h4&gt;
&lt;p&gt;The intuition behind this loss is that the target heatmap should exhibit high responses at the ground truth target keypoint positions while suppressing responses at other locations as much as possible. Furthermore, it doesn&amp;rsquo;t rely on any prior knowledge about the heatmap response distribution, preserving the potential for multi-modal distributions to handle matching ambiguities in challenging cases. To achieve this, we maximize the ratio between the response at the true positions and the sum of responses across the entire heatmap. This ratio constitutes the Relative Response (RR) loss.&lt;/p&gt;
&lt;h4 id=&#34;24-dense-feature-matching&#34;&gt;2.4 Dense Feature Matching&lt;/h4&gt;
&lt;p&gt;For each source keypoint position in the source image, the method described above is used to generate the corresponding target heatmap. The position with the maximum response value in the heatmap is selected as the estimated target keypoint location. Then, the descriptors at these estimated target keypoint positions undergo the same operation on the source descriptor mapping to estimate the source keypoint positions. Due to the nature of dense matching, traditional nearest neighbor criteria used in pairwise feature matching for local descriptors are overly strict. Hence, we relax these criteria, accepting matches as long as the estimated source keypoint positions are in the vicinity of the original source keypoint positions. This is referred to as the cycle consistency criterion. The computation for dense matching can be parallelized on the GPU, treating all sampled source descriptors as a kernel of size $N √ó L √ó 1 √ó 1$, where $N$ serves as the query count of source keypoint positions and is used as the output channel dimension, and $L$ represents the length of feature descriptors used as the input channel dimension for standard 2D convolutional operations.&lt;/p&gt;
&lt;h4 id=&#34;25-experiment&#34;&gt;2.5 Experiment&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;&lt;strong&gt;training loss&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/winning/images/2022-08-29-14-26-30_hu08a015447aedbc68c8e7b969c722f634_56229_5f6fe5b8011606b51a5c26d90dac0de0.webp 400w,
               /project/winning/images/2022-08-29-14-26-30_hu08a015447aedbc68c8e7b969c722f634_56229_5ecef6f770997f20f4306dcdf94e7e60.webp 760w,
               /project/winning/images/2022-08-29-14-26-30_hu08a015447aedbc68c8e7b969c722f634_56229_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://EdgarFx.github.io/project/winning/images/2022-08-29-14-26-30_hu08a015447aedbc68c8e7b969c722f634_56229_5f6fe5b8011606b51a5c26d90dac0de0.webp&#34;
               width=&#34;760&#34;
               height=&#34;366&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ol start=&#34;2&#34;&gt;
&lt;li&gt;&lt;strong&gt;validation accuracy&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/winning/images/2022-08-29-14-27-13_hu3b6b0584364fdb7de68a5ed4981e81a2_146756_6c56734e97f74ab00bb405e463b9615f.webp 400w,
               /project/winning/images/2022-08-29-14-27-13_hu3b6b0584364fdb7de68a5ed4981e81a2_146756_846463119b7b1ff8934c7ebc298576f2.webp 760w,
               /project/winning/images/2022-08-29-14-27-13_hu3b6b0584364fdb7de68a5ed4981e81a2_146756_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://EdgarFx.github.io/project/winning/images/2022-08-29-14-27-13_hu3b6b0584364fdb7de68a5ed4981e81a2_146756_6c56734e97f74ab00bb405e463b9615f.webp&#34;
               width=&#34;760&#34;
               height=&#34;380&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;Note that the difference between the three accuracies here is that the thresholds are different.&lt;/p&gt;
&lt;ol start=&#34;3&#34;&gt;
&lt;li&gt;&lt;strong&gt;test accuracy&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/winning/images/2022-08-29-14-28-10_huef7f308cfecd340bdded5363038fb572_94655_c25ca82b79af54347c1ed0c9da4353b6.webp 400w,
               /project/winning/images/2022-08-29-14-28-10_huef7f308cfecd340bdded5363038fb572_94655_36d8048633462306c7c23b6c70d9c643.webp 760w,
               /project/winning/images/2022-08-29-14-28-10_huef7f308cfecd340bdded5363038fb572_94655_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://EdgarFx.github.io/project/winning/images/2022-08-29-14-28-10_huef7f308cfecd340bdded5363038fb572_94655_c25ca82b79af54347c1ed0c9da4353b6.webp&#34;
               width=&#34;760&#34;
               height=&#34;393&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;ol start=&#34;4&#34;&gt;
&lt;li&gt;&lt;strong&gt;Matching Results&lt;/strong&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;















&lt;figure  &gt;
  &lt;div class=&#34;d-flex justify-content-center&#34;&gt;
    &lt;div class=&#34;w-100&#34; &gt;&lt;img alt=&#34;&#34; srcset=&#34;
               /project/winning/images/2022-08-29-14-29-28_hu262a69f2317eeeb2ba1566cb3386cbc7_567646_e0ae7829e1aa753a8af71e361d7e35ff.webp 400w,
               /project/winning/images/2022-08-29-14-29-28_hu262a69f2317eeeb2ba1566cb3386cbc7_567646_cdcacecd2f6cc49d6ebd542ad4b28fca.webp 760w,
               /project/winning/images/2022-08-29-14-29-28_hu262a69f2317eeeb2ba1566cb3386cbc7_567646_1200x1200_fit_q75_h2_lanczos_3.webp 1200w&#34;
               src=&#34;https://EdgarFx.github.io/project/winning/images/2022-08-29-14-29-28_hu262a69f2317eeeb2ba1566cb3386cbc7_567646_e0ae7829e1aa753a8af71e361d7e35ff.webp&#34;
               width=&#34;760&#34;
               height=&#34;311&#34;
               loading=&#34;lazy&#34; data-zoomable /&gt;&lt;/div&gt;
  &lt;/div&gt;&lt;/figure&gt;
&lt;/p&gt;
&lt;p&gt;The first picture on the left here is a comparison of the feature matching of Deep Learning and sift, the second picture is a heat map, and the third picture is an illustration of the target map and the key point positions determined by the detector.&lt;/p&gt;
&lt;h2 id=&#34;references&#34;&gt;References:&lt;/h2&gt;
&lt;blockquote&gt;
&lt;p&gt;Dense Depth Estimation: &lt;a href=&#34;https://arxiv.org/pdf/1902.07766.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/1902.07766.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Dense Feature Descriptors: &lt;a href=&#34;https://arxiv.org/pdf/2003.00619.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2003.00619.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
&lt;blockquote&gt;
&lt;p&gt;Dense 3D Reconstruction: &lt;a href=&#34;https://arxiv.org/pdf/2003.08502.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;https://arxiv.org/pdf/2003.08502.pdf&lt;/a&gt;&lt;/p&gt;
&lt;/blockquote&gt;
</description>
    </item>
    
    <item>
      <title>QuickTable, an ultra lightweight application to extract tables in PDF</title>
      <link>https://EdgarFx.github.io/project/quick_table/</link>
      <pubDate>Sat, 20 Aug 2022 10:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/project/quick_table/</guid>
      <description>&lt;p&gt;This research proposes a new pipeline to extract tables of interest in PDF files, and develops an ultra lightweight application named QuickTable accordingly. Most of the previous research only focused on one or two tasks of table recognition and there is little research on finding tables of interest. The developed QuickTable uses the proposed pipeline based on PP-Picodet, SLANet, PPOCRv3, Text Segmentation and Cosine Similarity Analysis, which allows users to upload PDF files &lt;strong&gt;from mobile devices&lt;/strong&gt; and &lt;strong&gt;enter keywords to get tables of interest&lt;/strong&gt;. In addition, we have trained models in both Chinese and English so that users can upload files in &lt;strong&gt;different languages&lt;/strong&gt;. Experiments show that the proposed pipeline is lightweight and outperforms previous approaches, demonstrating the effectiveness of our method. The &lt;a href=&#34;https://github.com/EdgarFx/QuickTable&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;code&lt;/a&gt; is open-sourced.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>2022 Carbon Rating Report of China&#39;s 100 Overseas Listed Companies</title>
      <link>https://EdgarFx.github.io/publication/carbon_rating/</link>
      <pubDate>Fri, 17 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/publication/carbon_rating/</guid>
      <description></description>
    </item>
    
    <item>
      <title>2022 Carbon Rating Report of China&#39;s 100 Overseas Listed Companies</title>
      <link>https://EdgarFx.github.io/research/carbon_rating/</link>
      <pubDate>Fri, 17 Jun 2022 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/research/carbon_rating/</guid>
      <description>&lt;p&gt;At present, there is no unified definition of the organizational boundary, scope and management for corporate carbon emission information disclosure in China. China-based Stock Exchanges have no mandatory disclosure requirements for carbon emission information of listed companies either. In the absence of uniform standards, there may be inconsistencies in the statistical methodologies of corporate carbon information disclosure, resulting in the lack of comparability among different emission data. Domestic regulatory authorities need to establish a unified corporate carbon information disclosure standard to supervise the carbon information disclosure of listed companies and assist companies, thus to carry out standardized disclosure. Carbon disclosure is now of great practical significance to the development of enterprises. The leading enterprises in the capital market generally have higher disclosure levels, which will be in turn incentivized by the stock market, thus forming a positive cycle. At the same time, with the national carbon peaking and carbon neutrality policy in place, carbon disclosure, as a social and environmental factor, will continuously be the focus of the capital market. It will also have a greater impact on a company‚Äôs stock price, profits and other financial performances. Therefore, it is even more imperative for all companies to improve their own carbon emission management and disclosure systems. While achieving emission reduction targets, they should also reduce the carbon emission intensity and improve the quality of carbon information disclosure. This report proposes a carbon rating system and releases the carbon rating of China‚Äôs 100 Overseas Listed Companies.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>News</title>
      <link>https://EdgarFx.github.io/news/</link>
      <pubDate>Fri, 01 Dec 2017 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/news/</guid>
      <description>&lt;!-- [**Note**] This is a &#34;non-discriminating&#34; list of news, including the good **and the bad**. If you&#39;re interested, check out my blog post on [**Why I share my paper rejection.**](https://medium.com/@yixue_zhao/why-i-share-my-paper-rejection-from-a-junior-researchers-perspective-81472e3d7d2a?source=friends_link&amp;sk=0758c50833fdf012ff3aae59e79697e8) --&gt;









&lt;p&gt;[&lt;strong&gt;Jun 2025&lt;/strong&gt;] Our paper &lt;strong&gt;Bag of Word Groups (BoWG): A Robust and Efficient Loop Closure Detection Method Under Perceptual Aliasing&lt;/strong&gt; has been accepted by IROS 2025 ü•≥.&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Aug 2024&lt;/strong&gt;] Our paper &lt;a href=&#34;https://ietresearch.onlinelibrary.wiley.com/doi/10.1049/enc2.12122&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Carbon market risk estimation using quantum conditional generative adversarial network and amplitude estimation&lt;/strong&gt;&lt;/a&gt; is published by &lt;strong&gt;Energy Conversion and Economics&lt;/strong&gt;.&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Jul 2024&lt;/strong&gt;] Our paper &lt;a href=&#34;https://www.nature.com/articles/s41598-024-67922-w&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Power system fault diagnosis with quantum computing and efficient gate decomposition&lt;/strong&gt;&lt;/a&gt; is published by &lt;strong&gt;Scientific Report&lt;/strong&gt; (JCR Q1), which is my first paper as the first author during my undergraduate studies üòâ.&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;May 2024&lt;/strong&gt;] &lt;strong&gt;Undergraduate Graduation&lt;/strong&gt; üéâ! I graduated with the first class honours from &lt;a href=&#34;https://www.cuhk.edu.cn/en&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;The Chinese University of Hong Kong, Shenzhen&lt;/strong&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Apr 2024&lt;/strong&gt;] Waiwai is admitted by MS in Information Networking (MSIN) program at CMU, congratulations to her!!! We can go to CMU together ü•≥!&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Mar 2024&lt;/strong&gt;] Happy 1st anniversary to me and Waiwai ü•∞!&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Feb 2024&lt;/strong&gt;] Offer of the MS in Robotics (MSR) program at CMU! Becoming a robotics scientist in the future!&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Jan 2024&lt;/strong&gt;] Our paper &lt;strong&gt;Carbon Market Risk Estimation Using Quantum Generative Adversarial Network and Amplitude Estimation&lt;/strong&gt; is accepted by Energy Conversion and Economics.&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Aug 2023&lt;/strong&gt;] Finished the &lt;a href=&#34;https://riss.ri.cmu.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;RISS&lt;/strong&gt;&lt;/a&gt; summer research program at &lt;a href=&#34;https://www.cmu.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Carnegie Mellon University&lt;/strong&gt;&lt;/a&gt; üéä, also presented my &lt;a href=&#34;https://docs.google.com/presentation/d/1Fttcj0lzZ_AiARmjTG51l_Mxh_Jk9hSI/edit?usp=sharing&amp;amp;ouid=110083063639360259216&amp;amp;rtpof=true&amp;amp;sd=true&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;research poster&lt;/strong&gt;&lt;/a&gt; at the final showcase!&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Jul 2023&lt;/strong&gt;] Our paper &lt;a href=&#34;https://link.springer.com/article/10.1007/s42484-023-00115-2&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Linear-layer-enhanced quantum long short-term memory for carbon price forecasting&lt;/strong&gt;&lt;/a&gt; is published by &lt;strong&gt;Quantum Machine Intelligence&lt;/strong&gt;, which is my first published journal paper ü•≥!&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Mar 2023&lt;/strong&gt;] Together with Waiwai! Love forever ü•∞!&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Mar 2023&lt;/strong&gt;] Offer of the &lt;a href=&#34;https://riss.ri.cmu.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Robotics Institute Summer Scholars (RISS)&lt;/strong&gt;&lt;/a&gt; program at &lt;a href=&#34;https://www.cmu.edu/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;Carnegie Mellon University&lt;/strong&gt;&lt;/a&gt;! Looking forward to join CMU this summer ü§ù.&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Jan 2023&lt;/strong&gt;] Arrived at SFO, first time visiting the US, excited to study at UC Berkeley.&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Sep 2022&lt;/strong&gt;] Offer of the Berkeley Global Access Program of UC Berkeley! I will study at UCB in 2023 spring term.&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Sep 2022&lt;/strong&gt;] Serve as a TA of the Discrete Mathematics course.&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Jul 2022&lt;/strong&gt;] Begin my internship as a computer vision engineer at Winning Health Tech in Shanghai.&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Jun 2022&lt;/strong&gt;] Our academic report &lt;a href=&#34;https://airs.cuhk.edu.cn/files/2022-06/2022%20Carbon%20Rating%20Report%20of%20China%27s%20100%20Overseas%20Listed%20Companies_0.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;2022 Carbon Rating Report of China&amp;rsquo;s 100 Overseas Listed Companies&lt;/strong&gt;&lt;/a&gt; is published at the 2022 Global Forum on Sustainable Development.&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Jan 2022&lt;/strong&gt;] Offer of the Pembroke Summer Programme at &lt;a href=&#34;https://cam.ac.uk/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;University of Cambridge&lt;/strong&gt;&lt;/a&gt;, however, unable to join due to the epidemic&amp;hellip; sad&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Nov 2021&lt;/strong&gt;] Become the Head of the back-end development department at Teabreak Tech.&lt;/p&gt;
&lt;p&gt;[&lt;strong&gt;Dec 2020&lt;/strong&gt;] Joined a startup company Teabreak Tech (&lt;a href=&#34;https://github.com/TeaBreak-Tech&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;strong&gt;github&lt;/strong&gt;&lt;/a&gt;), working as a software back-end developer.&lt;/p&gt;

</description>
    </item>
    
    <item>
      <title></title>
      <link>https://EdgarFx.github.io/admin/config.yml</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/admin/config.yml</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://EdgarFx.github.io/project/navigation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/project/navigation/</guid>
      <description></description>
    </item>
    
    <item>
      <title></title>
      <link>https://EdgarFx.github.io/research/navigation/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>https://EdgarFx.github.io/research/navigation/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
